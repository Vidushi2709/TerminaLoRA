# TerminaLoRA 

## CLI Assistant â€“ Fine-Tuned TinyLlama for Terminal Automation

A lightweight language model fine-tuned using **LoRA** to act as a **CLI assistant** â€” turning natural language into actionable shell commands. Itâ€™s compact, fast, and trained on real-world Q\&A pairs for practical command-line tasks.

---

## ğŸš€ What It Does

* Accepts **natural language** CLI instructions.
* Responds with **minimal shell commands**.
* Supports **dry-run** mode and logs actions.
---

## ğŸ› ï¸ Fine-Tuning Details

| **Base Model**    | TinyLlama/TinyLlama-1.1B-Chat-v1.0       |
| ----------------- | ---------------------------------------- |
| **Fine-Tuning**   | LoRA (Low-Rank Adaptation) using ğŸ¤— PEFT |
| **Epochs**        | **3**                                    |
| **Precision**     | 8-bit optimizer (`paged_adamw_8bit`)     |
| **Platform**      | Kaggle T4 GPU                            |
| **Training Time** | \~45 minutes                             |
| **Adapter Size**  | \~8.61 MB                                |

---
## ğŸ“‰ Training Loss Curve

This graph shows the training loss over time during the 3-epoch fine-tuning of the CLI Assistant model.

![Training Loss Curve](loss_loss_loss.png)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                      # Cleaned CLI Q&A dataset (~150 pairs)
â”œâ”€â”€ my_finetuned_model/       # LoRA adapter weights
â”œâ”€â”€ cli_agent.py              # CLI Assistant (inference interface)
â”œâ”€â”€ training.ipynb            # LoRA fine-tuning notebook
â”œâ”€â”€ logs/                     # CLI session logs
â”‚   â””â”€â”€ trace.jsonl
```

---

## ğŸ§¾ Data Source

* **StackExchange SQL Query**
  Extracted high-quality Q\&A pairs tagged with `bash`, `git`, `grep`.

```sql
SELECT TOP 150
  Posts.Title,
  Posts.Body,
  Answers.Body AS Answer
FROM Posts
JOIN Posts AS Answers ON Posts.AcceptedAnswerId = Answers.Id
WHERE Posts.Tags LIKE '%bash%' OR Posts.Tags LIKE '%git%' OR Posts.Tags LIKE '%grep%'
AND Posts.PostTypeId = 1
ORDER BY Posts.Score DESC
```

* Preprocessed and saved to `data/data_for_finetuning.json`.

---

## ğŸ¤– Run the CLI Agent

```bash
python cli_agent.py
```

Example usage:

```bash
ğŸ§  Enter your CLI instruction:
> Check if Python is installed and show version

ğŸ’¡ AI Plan:
1. python --version

ğŸ’» Dry-run Command:
echo python --version

ğŸ“„ Logged to: logs/trace.jsonl
```

---

## ğŸ”® Future Work

* **Longer training**: Further epochs may improve ROUGE/BLEU metrics.
* **Multilingual CLI**: Hindi/Spanish tech command generation.
* **Humor-Tuned LLM**: Training with casual, humorous CLI instructions to increase user engagement.

---

## ğŸ Conclusion

This project proves that even **compact models** like TinyLlama can be fine-tuned to perform real-world tasks using LoRA with minimal compute and time. The assistant is fast, effective, and deployable in lightweight environments.

---

created by Vin â¤ï¸
